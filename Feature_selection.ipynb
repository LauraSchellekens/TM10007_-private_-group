{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature_selection.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOfJyNhExE41fXHOBDJy3E2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LauraSchellekens/TM10007_-private_-group/blob/Manon_feature_selection/Feature_selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yspojLoynk8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "800f7dcb-3e77-4aa6-dd7c-aa67fa104c1f"
      },
      "source": [
        "# Run this to use from colab environment\n",
        "!pip install -q --upgrade git+https://github.com/karinvangarderen/tm10007_project.git\n",
        "!pip install sklearn numpy matplotlib"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for brats (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhTSxoj3oma-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General packages not sure anymore what I'm actually using lol\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import model_selection\n",
        "from sklearn import preprocessing\n",
        "from sklearn import impute\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn import svm\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import feature_selection \n",
        "from sklearn import feature_selection \n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qrk3dpP9pv6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9d7d97c5-6ccc-40e2-8a6b-500f892b49ef"
      },
      "source": [
        "# Data loading functions. Uncomment the one you want to use\n",
        "from adni.load_data import load_data\n",
        "\n",
        "data = load_data()\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of columns: {len(data.columns)}')\n",
        "\n",
        "\n",
        "X = data.drop('label', axis=1) # All data without column 'label'\n",
        "Y = data['label'] \n",
        "Y.replace(('AD', 'CN'), (1, 0), inplace=True) # convert AD and CN to 1 and 0"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of samples: 855\n",
            "The number of columns: 268\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE3iVgNppzrP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "9217135c-0deb-45bd-9629-f6082034bf2c"
      },
      "source": [
        "# split into train (70%) and test (30%) set\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3)\n",
        "\n",
        "print (X_train.shape, y_train.shape)\n",
        "print (X_test.shape, y_test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(598, 267) (598,)\n",
            "(257, 267) (257,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHpvbroXp511",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "077745b0-640b-4bcd-d49d-d1c9c8af4ceb"
      },
      "source": [
        "# Preprocessing: drop feature if too many missing values\n",
        "\n",
        "X_train = X_train.replace(0, np.NaN)                  # replace 0 with NaN\n",
        "missing_per_feature = X_train.isnull().sum()          # gives the amount of missing values (NaN) per feature\n",
        "pct_null = missing_per_feature / len(X_train)         # gives percentage of missing values per feature\n",
        "missing_features = pct_null[pct_null > 0.40].index    # gives features with more than 40% missing values\n",
        "X_train.drop(missing_features, axis=1, inplace=True)  # remove feature if more than 40% missing values\n",
        "\n",
        "missing_per_feature_max = max(X_train.isnull().sum()) # gives the maximum amount of missing values (NaN) per feature AFTER dropping incomplete features\n",
        "\n",
        "print(missing_per_feature_max)\n",
        "print (X_train.shape, y_train.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "102\n",
            "(598, 261) (598,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4jbZlJNp91q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "588dcdcb-2815-4660-ed84-0ad04624b627"
      },
      "source": [
        "# Preprocessing:  impute missing values (fill missing)\n",
        "\n",
        "imputer = impute.SimpleImputer(strategy='mean')     # imputer with mean \n",
        "X_train_imp = imputer.fit_transform(X_train)          # impute  \n",
        "X_train_imp = pd.DataFrame(data=X_train_imp, index=[X_train.index], columns=[X_train.columns]) # turn created np.array back to pandas df\n",
        "\n",
        "type(X_train_imp)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mYOiIpqSTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "outputId": "13326c52-9e19-48c6-fa10-0c753fd07a09"
      },
      "source": [
        "# Preprocessing: scaling either standard or robust (removes median and scales data according to quantile range) \n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "scaler.fit_transform(X_train_imp)\n",
        "X_train_scaled = scaler.transform(X_train_imp)\n",
        "# robust = preprocessing.RobustScaler()\n",
        "# robust.fit_transform(X_train_imp)\n",
        "# X_train_scaled = robust.transform(X_train_imp)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(data=X_train_scaled, index=[X_train.index], columns=[X_train.columns]) # turn created np.array back to pandas df\n",
        "X_train_scaled\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>hf_energy</th>\n",
              "      <th>hf_entropy</th>\n",
              "      <th>hf_kurtosis</th>\n",
              "      <th>hf_max</th>\n",
              "      <th>hf_mean</th>\n",
              "      <th>hf_median</th>\n",
              "      <th>hf_min</th>\n",
              "      <th>hf_peak</th>\n",
              "      <th>hf_quartile_range</th>\n",
              "      <th>hf_range</th>\n",
              "      <th>hf_skewness</th>\n",
              "      <th>hf_std</th>\n",
              "      <th>logf_energy_sigma1</th>\n",
              "      <th>logf_energy_sigma10</th>\n",
              "      <th>logf_energy_sigma5</th>\n",
              "      <th>logf_entropy_sigma1</th>\n",
              "      <th>logf_entropy_sigma10</th>\n",
              "      <th>logf_entropy_sigma5</th>\n",
              "      <th>logf_kurtosis_sigma1</th>\n",
              "      <th>logf_kurtosis_sigma10</th>\n",
              "      <th>logf_kurtosis_sigma5</th>\n",
              "      <th>logf_max_sigma1</th>\n",
              "      <th>logf_max_sigma10</th>\n",
              "      <th>logf_max_sigma5</th>\n",
              "      <th>logf_mean_sigma1</th>\n",
              "      <th>logf_mean_sigma10</th>\n",
              "      <th>logf_mean_sigma5</th>\n",
              "      <th>logf_median_sigma1</th>\n",
              "      <th>logf_median_sigma10</th>\n",
              "      <th>logf_median_sigma5</th>\n",
              "      <th>logf_min_sigma1</th>\n",
              "      <th>logf_min_sigma10</th>\n",
              "      <th>logf_min_sigma5</th>\n",
              "      <th>logf_peak_sigma1</th>\n",
              "      <th>logf_peak_sigma10</th>\n",
              "      <th>logf_peak_sigma5</th>\n",
              "      <th>logf_quartile_range_sigma1</th>\n",
              "      <th>logf_quartile_range_sigma10</th>\n",
              "      <th>logf_quartile_range_sigma5</th>\n",
              "      <th>logf_range_sigma1</th>\n",
              "      <th>...</th>\n",
              "      <th>tf_NGTDM_Coarseness</th>\n",
              "      <th>tf_NGTDM_Complexity</th>\n",
              "      <th>tf_NGTDM_Contrast</th>\n",
              "      <th>tf_NGTDM_Strength</th>\n",
              "      <th>vf_Frangi_edge_energy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_entropy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_kurtosis_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_max_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_mean_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_median_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_min_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_peak_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_quartile_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_skewness_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_edge_std_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_energy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_entropy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_kurtosis_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_max_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_mean_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_median_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_min_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_peak_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_quartile_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_skewness_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_full_std_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_energy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_entropy_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_kurtosis_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_max_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_mean_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_median_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_min_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_peak_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_quartile_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_range_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_skewness_SR(1.0, 10.0)_SS2.0</th>\n",
              "      <th>vf_Frangi_inner_std_SR(1.0, 10.0)_SS2.0</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ID</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_021_S_0642_bl_0</th>\n",
              "      <td>-0.160235</td>\n",
              "      <td>1.656363</td>\n",
              "      <td>-1.461417</td>\n",
              "      <td>-0.194924</td>\n",
              "      <td>-1.173729</td>\n",
              "      <td>-0.518043</td>\n",
              "      <td>-1.644104</td>\n",
              "      <td>-0.251573</td>\n",
              "      <td>2.366362</td>\n",
              "      <td>1.698972</td>\n",
              "      <td>0.807054</td>\n",
              "      <td>2.361114</td>\n",
              "      <td>0.350970</td>\n",
              "      <td>-0.144765</td>\n",
              "      <td>-0.146918</td>\n",
              "      <td>0.507771</td>\n",
              "      <td>1.353871</td>\n",
              "      <td>1.092985</td>\n",
              "      <td>-0.678713</td>\n",
              "      <td>-0.936874</td>\n",
              "      <td>-1.002432</td>\n",
              "      <td>-0.814865</td>\n",
              "      <td>1.780225</td>\n",
              "      <td>2.087918</td>\n",
              "      <td>-0.040429</td>\n",
              "      <td>1.416965</td>\n",
              "      <td>1.118795</td>\n",
              "      <td>1.018903</td>\n",
              "      <td>1.397031</td>\n",
              "      <td>1.194431</td>\n",
              "      <td>0.589138</td>\n",
              "      <td>-0.078270</td>\n",
              "      <td>0.242154</td>\n",
              "      <td>-0.302968</td>\n",
              "      <td>-0.274393</td>\n",
              "      <td>-0.465838</td>\n",
              "      <td>0.106534</td>\n",
              "      <td>1.887010</td>\n",
              "      <td>1.561567</td>\n",
              "      <td>-0.753357</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>1.114315e+00</td>\n",
              "      <td>1.376069</td>\n",
              "      <td>-0.436705</td>\n",
              "      <td>3.767620</td>\n",
              "      <td>1.007494</td>\n",
              "      <td>-0.858368</td>\n",
              "      <td>2.481822</td>\n",
              "      <td>2.248571</td>\n",
              "      <td>1.448393</td>\n",
              "      <td>0.720203</td>\n",
              "      <td>0.426721</td>\n",
              "      <td>2.052465</td>\n",
              "      <td>2.486391</td>\n",
              "      <td>-0.305571</td>\n",
              "      <td>2.619735</td>\n",
              "      <td>3.767620</td>\n",
              "      <td>1.007494</td>\n",
              "      <td>-0.858368</td>\n",
              "      <td>2.481822</td>\n",
              "      <td>2.248571</td>\n",
              "      <td>1.448393</td>\n",
              "      <td>0.720203</td>\n",
              "      <td>0.426721</td>\n",
              "      <td>2.052465</td>\n",
              "      <td>2.486391</td>\n",
              "      <td>-0.305571</td>\n",
              "      <td>2.619735</td>\n",
              "      <td>4.757970e-01</td>\n",
              "      <td>1.090266</td>\n",
              "      <td>-0.331174</td>\n",
              "      <td>0.716762</td>\n",
              "      <td>8.366559e-01</td>\n",
              "      <td>8.653206e-01</td>\n",
              "      <td>-0.668074</td>\n",
              "      <td>0.262689</td>\n",
              "      <td>7.285633e-01</td>\n",
              "      <td>0.833764</td>\n",
              "      <td>-6.356775e-01</td>\n",
              "      <td>0.718112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_135_S_4566_bl_0</th>\n",
              "      <td>-0.151509</td>\n",
              "      <td>-1.080483</td>\n",
              "      <td>0.650771</td>\n",
              "      <td>-0.598985</td>\n",
              "      <td>0.129008</td>\n",
              "      <td>-0.080460</td>\n",
              "      <td>0.919515</td>\n",
              "      <td>0.304470</td>\n",
              "      <td>-0.776296</td>\n",
              "      <td>-1.478649</td>\n",
              "      <td>-0.012291</td>\n",
              "      <td>-1.223840</td>\n",
              "      <td>1.135589</td>\n",
              "      <td>2.073184</td>\n",
              "      <td>0.166404</td>\n",
              "      <td>-1.444466</td>\n",
              "      <td>-2.212356</td>\n",
              "      <td>-1.020959</td>\n",
              "      <td>1.572426</td>\n",
              "      <td>2.474097</td>\n",
              "      <td>0.862103</td>\n",
              "      <td>0.473729</td>\n",
              "      <td>-0.377544</td>\n",
              "      <td>-0.520251</td>\n",
              "      <td>-0.029652</td>\n",
              "      <td>0.148016</td>\n",
              "      <td>-0.380564</td>\n",
              "      <td>0.632269</td>\n",
              "      <td>0.436452</td>\n",
              "      <td>-0.191677</td>\n",
              "      <td>-1.357495</td>\n",
              "      <td>-0.049057</td>\n",
              "      <td>-0.472530</td>\n",
              "      <td>1.247069</td>\n",
              "      <td>0.929014</td>\n",
              "      <td>1.089017</td>\n",
              "      <td>-1.069410</td>\n",
              "      <td>-0.508104</td>\n",
              "      <td>-1.038297</td>\n",
              "      <td>1.149759</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253094</td>\n",
              "      <td>-9.424061e-01</td>\n",
              "      <td>-0.566493</td>\n",
              "      <td>0.662380</td>\n",
              "      <td>-0.784234</td>\n",
              "      <td>-0.405931</td>\n",
              "      <td>0.634856</td>\n",
              "      <td>-1.007747</td>\n",
              "      <td>-1.177728</td>\n",
              "      <td>-1.143652</td>\n",
              "      <td>-0.718950</td>\n",
              "      <td>0.185360</td>\n",
              "      <td>-1.046934</td>\n",
              "      <td>-0.992557</td>\n",
              "      <td>0.653745</td>\n",
              "      <td>-1.034316</td>\n",
              "      <td>-0.784234</td>\n",
              "      <td>-0.405931</td>\n",
              "      <td>0.634856</td>\n",
              "      <td>-1.007747</td>\n",
              "      <td>-1.177728</td>\n",
              "      <td>-1.143652</td>\n",
              "      <td>-0.718950</td>\n",
              "      <td>0.185360</td>\n",
              "      <td>-1.046934</td>\n",
              "      <td>-0.992557</td>\n",
              "      <td>0.653745</td>\n",
              "      <td>-1.034316</td>\n",
              "      <td>-2.877971e-01</td>\n",
              "      <td>-0.072593</td>\n",
              "      <td>-0.354456</td>\n",
              "      <td>-1.072831</td>\n",
              "      <td>-1.088411e+00</td>\n",
              "      <td>-9.692346e-01</td>\n",
              "      <td>-0.667930</td>\n",
              "      <td>-0.438268</td>\n",
              "      <td>-1.027272e+00</td>\n",
              "      <td>-1.036495</td>\n",
              "      <td>-1.891481e-01</td>\n",
              "      <td>-1.061634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_051_S_1123_bl_0</th>\n",
              "      <td>-0.529985</td>\n",
              "      <td>-0.933393</td>\n",
              "      <td>1.135272</td>\n",
              "      <td>-0.114889</td>\n",
              "      <td>0.337767</td>\n",
              "      <td>0.210209</td>\n",
              "      <td>0.582455</td>\n",
              "      <td>-0.931181</td>\n",
              "      <td>-0.753027</td>\n",
              "      <td>-0.739188</td>\n",
              "      <td>-1.005776</td>\n",
              "      <td>-0.821378</td>\n",
              "      <td>-1.230750</td>\n",
              "      <td>-0.987408</td>\n",
              "      <td>-0.695371</td>\n",
              "      <td>0.797753</td>\n",
              "      <td>0.525998</td>\n",
              "      <td>0.347346</td>\n",
              "      <td>-0.418636</td>\n",
              "      <td>-0.747656</td>\n",
              "      <td>-0.343633</td>\n",
              "      <td>-0.849043</td>\n",
              "      <td>1.203430</td>\n",
              "      <td>0.656317</td>\n",
              "      <td>0.350213</td>\n",
              "      <td>1.405845</td>\n",
              "      <td>1.719705</td>\n",
              "      <td>0.432327</td>\n",
              "      <td>0.296575</td>\n",
              "      <td>1.284030</td>\n",
              "      <td>-0.325909</td>\n",
              "      <td>0.569083</td>\n",
              "      <td>1.336303</td>\n",
              "      <td>-1.299722</td>\n",
              "      <td>-0.976810</td>\n",
              "      <td>-1.085842</td>\n",
              "      <td>0.043671</td>\n",
              "      <td>0.579034</td>\n",
              "      <td>0.592254</td>\n",
              "      <td>-0.125928</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>-7.319189e-01</td>\n",
              "      <td>-0.555305</td>\n",
              "      <td>-0.059244</td>\n",
              "      <td>-0.857033</td>\n",
              "      <td>1.071431</td>\n",
              "      <td>-0.694982</td>\n",
              "      <td>-1.152861</td>\n",
              "      <td>-1.037869</td>\n",
              "      <td>-0.942094</td>\n",
              "      <td>-0.572594</td>\n",
              "      <td>-1.349007</td>\n",
              "      <td>-0.913616</td>\n",
              "      <td>-1.145740</td>\n",
              "      <td>-0.795328</td>\n",
              "      <td>-1.110642</td>\n",
              "      <td>-0.857033</td>\n",
              "      <td>1.071431</td>\n",
              "      <td>-0.694982</td>\n",
              "      <td>-1.152861</td>\n",
              "      <td>-1.037869</td>\n",
              "      <td>-0.942094</td>\n",
              "      <td>-0.572594</td>\n",
              "      <td>-1.349007</td>\n",
              "      <td>-0.913616</td>\n",
              "      <td>-1.145740</td>\n",
              "      <td>-0.795328</td>\n",
              "      <td>-1.110642</td>\n",
              "      <td>-3.875133e-17</td>\n",
              "      <td>-2.401041</td>\n",
              "      <td>-0.933351</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.777093e-16</td>\n",
              "      <td>1.479763e-16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.622730</td>\n",
              "      <td>1.601795e-16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.337686e-16</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_073_S_0311_bl_0</th>\n",
              "      <td>-0.577616</td>\n",
              "      <td>0.047723</td>\n",
              "      <td>0.171698</td>\n",
              "      <td>0.371546</td>\n",
              "      <td>0.274125</td>\n",
              "      <td>0.225548</td>\n",
              "      <td>-0.375730</td>\n",
              "      <td>-1.168014</td>\n",
              "      <td>-0.256702</td>\n",
              "      <td>0.698837</td>\n",
              "      <td>-0.517832</td>\n",
              "      <td>0.068544</td>\n",
              "      <td>-0.154455</td>\n",
              "      <td>-1.200638</td>\n",
              "      <td>-1.307839</td>\n",
              "      <td>-0.226239</td>\n",
              "      <td>1.261097</td>\n",
              "      <td>1.094006</td>\n",
              "      <td>0.452453</td>\n",
              "      <td>-0.856205</td>\n",
              "      <td>-0.939742</td>\n",
              "      <td>1.814198</td>\n",
              "      <td>1.004472</td>\n",
              "      <td>1.612523</td>\n",
              "      <td>1.449683</td>\n",
              "      <td>0.254539</td>\n",
              "      <td>1.060548</td>\n",
              "      <td>-0.285876</td>\n",
              "      <td>-0.073636</td>\n",
              "      <td>0.581126</td>\n",
              "      <td>1.036139</td>\n",
              "      <td>0.917253</td>\n",
              "      <td>0.704475</td>\n",
              "      <td>-0.750663</td>\n",
              "      <td>-1.033623</td>\n",
              "      <td>-1.051936</td>\n",
              "      <td>-0.011772</td>\n",
              "      <td>1.136839</td>\n",
              "      <td>0.951613</td>\n",
              "      <td>0.030819</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>-4.208521e-01</td>\n",
              "      <td>-0.483938</td>\n",
              "      <td>-0.295593</td>\n",
              "      <td>-0.289387</td>\n",
              "      <td>1.129356</td>\n",
              "      <td>-0.966242</td>\n",
              "      <td>0.199401</td>\n",
              "      <td>0.425570</td>\n",
              "      <td>0.460629</td>\n",
              "      <td>0.165523</td>\n",
              "      <td>-1.211087</td>\n",
              "      <td>0.439201</td>\n",
              "      <td>0.194959</td>\n",
              "      <td>-0.783215</td>\n",
              "      <td>0.300147</td>\n",
              "      <td>-0.289387</td>\n",
              "      <td>1.129356</td>\n",
              "      <td>-0.966242</td>\n",
              "      <td>0.199401</td>\n",
              "      <td>0.425570</td>\n",
              "      <td>0.460629</td>\n",
              "      <td>0.165523</td>\n",
              "      <td>-1.211087</td>\n",
              "      <td>0.439201</td>\n",
              "      <td>0.194959</td>\n",
              "      <td>-0.783215</td>\n",
              "      <td>0.300147</td>\n",
              "      <td>-2.808164e-01</td>\n",
              "      <td>-0.460216</td>\n",
              "      <td>-0.616725</td>\n",
              "      <td>-0.597919</td>\n",
              "      <td>-2.112416e-01</td>\n",
              "      <td>-2.263529e-01</td>\n",
              "      <td>0.153835</td>\n",
              "      <td>-0.622730</td>\n",
              "      <td>-1.633891e-01</td>\n",
              "      <td>-0.698373</td>\n",
              "      <td>-8.693682e-01</td>\n",
              "      <td>-0.525310</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_037_S_4308_bl_0</th>\n",
              "      <td>-0.299787</td>\n",
              "      <td>-0.868119</td>\n",
              "      <td>0.708544</td>\n",
              "      <td>-0.228409</td>\n",
              "      <td>0.169528</td>\n",
              "      <td>0.031458</td>\n",
              "      <td>0.482289</td>\n",
              "      <td>1.076752</td>\n",
              "      <td>-0.647196</td>\n",
              "      <td>-0.711546</td>\n",
              "      <td>-0.465909</td>\n",
              "      <td>-0.804040</td>\n",
              "      <td>0.571430</td>\n",
              "      <td>0.126047</td>\n",
              "      <td>-0.276593</td>\n",
              "      <td>-0.881799</td>\n",
              "      <td>-0.332667</td>\n",
              "      <td>-0.187036</td>\n",
              "      <td>0.211208</td>\n",
              "      <td>-0.123140</td>\n",
              "      <td>0.030444</td>\n",
              "      <td>-0.429033</td>\n",
              "      <td>-0.050661</td>\n",
              "      <td>-0.329500</td>\n",
              "      <td>1.923905</td>\n",
              "      <td>0.047801</td>\n",
              "      <td>0.277316</td>\n",
              "      <td>1.126881</td>\n",
              "      <td>0.471212</td>\n",
              "      <td>-0.110448</td>\n",
              "      <td>0.731155</td>\n",
              "      <td>-0.181886</td>\n",
              "      <td>-0.504099</td>\n",
              "      <td>0.997881</td>\n",
              "      <td>0.572640</td>\n",
              "      <td>0.917063</td>\n",
              "      <td>-0.698367</td>\n",
              "      <td>-0.048629</td>\n",
              "      <td>-0.345667</td>\n",
              "      <td>-0.691863</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>-7.790124e-01</td>\n",
              "      <td>-0.557933</td>\n",
              "      <td>-0.178834</td>\n",
              "      <td>-0.808763</td>\n",
              "      <td>-2.891586</td>\n",
              "      <td>2.399074</td>\n",
              "      <td>-1.058984</td>\n",
              "      <td>-1.485133</td>\n",
              "      <td>-1.742385</td>\n",
              "      <td>-0.718951</td>\n",
              "      <td>3.594585</td>\n",
              "      <td>-1.228086</td>\n",
              "      <td>-1.044510</td>\n",
              "      <td>2.244062</td>\n",
              "      <td>-1.001987</td>\n",
              "      <td>-0.808763</td>\n",
              "      <td>-2.891586</td>\n",
              "      <td>2.399074</td>\n",
              "      <td>-1.058984</td>\n",
              "      <td>-1.485133</td>\n",
              "      <td>-1.742385</td>\n",
              "      <td>-0.718951</td>\n",
              "      <td>3.594585</td>\n",
              "      <td>-1.228086</td>\n",
              "      <td>-1.044510</td>\n",
              "      <td>2.244062</td>\n",
              "      <td>-1.001987</td>\n",
              "      <td>-2.847071e-01</td>\n",
              "      <td>-1.185090</td>\n",
              "      <td>1.986230</td>\n",
              "      <td>-0.629568</td>\n",
              "      <td>-1.064536e+00</td>\n",
              "      <td>-1.130078e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.143128</td>\n",
              "      <td>-1.071889e+00</td>\n",
              "      <td>-0.573231</td>\n",
              "      <td>2.430173e+00</td>\n",
              "      <td>-0.512715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_033_S_5235_bl_0</th>\n",
              "      <td>-0.009206</td>\n",
              "      <td>-0.888327</td>\n",
              "      <td>0.670816</td>\n",
              "      <td>0.323670</td>\n",
              "      <td>0.604240</td>\n",
              "      <td>0.437308</td>\n",
              "      <td>0.980811</td>\n",
              "      <td>-0.025037</td>\n",
              "      <td>-0.640813</td>\n",
              "      <td>-0.858752</td>\n",
              "      <td>-0.007477</td>\n",
              "      <td>-0.925095</td>\n",
              "      <td>-0.396600</td>\n",
              "      <td>-0.171989</td>\n",
              "      <td>-0.115081</td>\n",
              "      <td>-1.523602</td>\n",
              "      <td>-0.676644</td>\n",
              "      <td>-1.166218</td>\n",
              "      <td>0.408278</td>\n",
              "      <td>0.145607</td>\n",
              "      <td>0.129649</td>\n",
              "      <td>-0.342459</td>\n",
              "      <td>-0.581378</td>\n",
              "      <td>-0.502628</td>\n",
              "      <td>0.311587</td>\n",
              "      <td>0.181312</td>\n",
              "      <td>-0.330289</td>\n",
              "      <td>-0.345071</td>\n",
              "      <td>0.328206</td>\n",
              "      <td>-0.208234</td>\n",
              "      <td>0.156062</td>\n",
              "      <td>-0.211967</td>\n",
              "      <td>-0.340111</td>\n",
              "      <td>1.255516</td>\n",
              "      <td>0.107805</td>\n",
              "      <td>1.062376</td>\n",
              "      <td>-1.621769</td>\n",
              "      <td>-0.593738</td>\n",
              "      <td>-0.870183</td>\n",
              "      <td>-0.252420</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>-8.834837e-01</td>\n",
              "      <td>-0.565358</td>\n",
              "      <td>0.240039</td>\n",
              "      <td>-0.514965</td>\n",
              "      <td>-2.135945</td>\n",
              "      <td>0.811127</td>\n",
              "      <td>-0.060458</td>\n",
              "      <td>-0.805531</td>\n",
              "      <td>-1.316418</td>\n",
              "      <td>-0.718951</td>\n",
              "      <td>2.323129</td>\n",
              "      <td>-0.391018</td>\n",
              "      <td>-0.032025</td>\n",
              "      <td>1.199874</td>\n",
              "      <td>-0.098717</td>\n",
              "      <td>-0.514965</td>\n",
              "      <td>-2.135945</td>\n",
              "      <td>0.811127</td>\n",
              "      <td>-0.060458</td>\n",
              "      <td>-0.805531</td>\n",
              "      <td>-1.316418</td>\n",
              "      <td>-0.718951</td>\n",
              "      <td>2.323129</td>\n",
              "      <td>-0.391018</td>\n",
              "      <td>-0.032025</td>\n",
              "      <td>1.199874</td>\n",
              "      <td>-0.098717</td>\n",
              "      <td>-2.314052e-01</td>\n",
              "      <td>-1.532265</td>\n",
              "      <td>2.419815</td>\n",
              "      <td>0.387914</td>\n",
              "      <td>-1.005263e+00</td>\n",
              "      <td>-1.127637e+00</td>\n",
              "      <td>-0.668075</td>\n",
              "      <td>2.734484</td>\n",
              "      <td>-1.230663e+00</td>\n",
              "      <td>0.490098</td>\n",
              "      <td>2.821960e+00</td>\n",
              "      <td>0.096168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_052_S_1251_bl_0</th>\n",
              "      <td>2.997111</td>\n",
              "      <td>-0.898065</td>\n",
              "      <td>0.316675</td>\n",
              "      <td>0.901514</td>\n",
              "      <td>1.205022</td>\n",
              "      <td>0.953441</td>\n",
              "      <td>2.017021</td>\n",
              "      <td>0.829622</td>\n",
              "      <td>-0.802723</td>\n",
              "      <td>-1.589941</td>\n",
              "      <td>1.579188</td>\n",
              "      <td>-1.271829</td>\n",
              "      <td>-0.412547</td>\n",
              "      <td>0.245688</td>\n",
              "      <td>1.500240</td>\n",
              "      <td>0.976041</td>\n",
              "      <td>0.017094</td>\n",
              "      <td>0.247533</td>\n",
              "      <td>-0.852694</td>\n",
              "      <td>-0.332330</td>\n",
              "      <td>-0.064683</td>\n",
              "      <td>-0.002819</td>\n",
              "      <td>1.050036</td>\n",
              "      <td>-0.054692</td>\n",
              "      <td>-0.810899</td>\n",
              "      <td>-0.770454</td>\n",
              "      <td>-0.812129</td>\n",
              "      <td>-0.597028</td>\n",
              "      <td>-1.391064</td>\n",
              "      <td>-0.170986</td>\n",
              "      <td>-0.183305</td>\n",
              "      <td>-0.312221</td>\n",
              "      <td>-0.937351</td>\n",
              "      <td>-0.619733</td>\n",
              "      <td>-0.413843</td>\n",
              "      <td>-0.603885</td>\n",
              "      <td>1.248964</td>\n",
              "      <td>1.162488</td>\n",
              "      <td>0.586963</td>\n",
              "      <td>0.127370</td>\n",
              "      <td>...</td>\n",
              "      <td>3.951090</td>\n",
              "      <td>1.483053e-16</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.084886</td>\n",
              "      <td>1.030097</td>\n",
              "      <td>-1.166343</td>\n",
              "      <td>-0.318243</td>\n",
              "      <td>0.043867</td>\n",
              "      <td>0.336362</td>\n",
              "      <td>-0.718571</td>\n",
              "      <td>-0.280122</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>-0.293429</td>\n",
              "      <td>-1.417388</td>\n",
              "      <td>-0.149992</td>\n",
              "      <td>-0.084886</td>\n",
              "      <td>1.030097</td>\n",
              "      <td>-1.166343</td>\n",
              "      <td>-0.318243</td>\n",
              "      <td>0.043867</td>\n",
              "      <td>0.336362</td>\n",
              "      <td>-0.718571</td>\n",
              "      <td>-0.280122</td>\n",
              "      <td>0.404211</td>\n",
              "      <td>-0.293429</td>\n",
              "      <td>-1.417388</td>\n",
              "      <td>-0.149992</td>\n",
              "      <td>-2.728272e-01</td>\n",
              "      <td>0.333631</td>\n",
              "      <td>0.387880</td>\n",
              "      <td>-0.661247</td>\n",
              "      <td>-9.247229e-01</td>\n",
              "      <td>-8.798897e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.336474</td>\n",
              "      <td>-7.097440e-01</td>\n",
              "      <td>-0.606338</td>\n",
              "      <td>7.356086e-01</td>\n",
              "      <td>-0.684797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_082_S_4208_bl_0</th>\n",
              "      <td>0.209651</td>\n",
              "      <td>-0.495808</td>\n",
              "      <td>0.256260</td>\n",
              "      <td>0.212503</td>\n",
              "      <td>0.500872</td>\n",
              "      <td>0.344807</td>\n",
              "      <td>0.734760</td>\n",
              "      <td>-0.447218</td>\n",
              "      <td>-0.588569</td>\n",
              "      <td>-0.665690</td>\n",
              "      <td>0.007223</td>\n",
              "      <td>-0.764215</td>\n",
              "      <td>0.002199</td>\n",
              "      <td>-0.176976</td>\n",
              "      <td>-0.112403</td>\n",
              "      <td>-1.504146</td>\n",
              "      <td>-1.278207</td>\n",
              "      <td>-1.231604</td>\n",
              "      <td>1.225884</td>\n",
              "      <td>0.215872</td>\n",
              "      <td>0.641440</td>\n",
              "      <td>-0.525754</td>\n",
              "      <td>-0.068167</td>\n",
              "      <td>-0.527911</td>\n",
              "      <td>-0.884227</td>\n",
              "      <td>0.309532</td>\n",
              "      <td>-0.165161</td>\n",
              "      <td>-0.024858</td>\n",
              "      <td>0.377143</td>\n",
              "      <td>-0.212540</td>\n",
              "      <td>-0.869959</td>\n",
              "      <td>0.412523</td>\n",
              "      <td>-0.294051</td>\n",
              "      <td>0.752916</td>\n",
              "      <td>0.619124</td>\n",
              "      <td>0.858937</td>\n",
              "      <td>-1.495497</td>\n",
              "      <td>-0.801198</td>\n",
              "      <td>-1.369390</td>\n",
              "      <td>0.390575</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>-8.852725e-01</td>\n",
              "      <td>-0.565164</td>\n",
              "      <td>0.269456</td>\n",
              "      <td>-0.575973</td>\n",
              "      <td>-1.063834</td>\n",
              "      <td>0.233854</td>\n",
              "      <td>-0.387013</td>\n",
              "      <td>-0.548093</td>\n",
              "      <td>-0.604287</td>\n",
              "      <td>-0.667488</td>\n",
              "      <td>-0.383563</td>\n",
              "      <td>-0.432997</td>\n",
              "      <td>-0.365270</td>\n",
              "      <td>0.311565</td>\n",
              "      <td>-0.424887</td>\n",
              "      <td>-0.575973</td>\n",
              "      <td>-1.063834</td>\n",
              "      <td>0.233854</td>\n",
              "      <td>-0.387013</td>\n",
              "      <td>-0.548093</td>\n",
              "      <td>-0.604287</td>\n",
              "      <td>-0.667488</td>\n",
              "      <td>-0.383563</td>\n",
              "      <td>-0.432997</td>\n",
              "      <td>-0.365270</td>\n",
              "      <td>0.311565</td>\n",
              "      <td>-0.424887</td>\n",
              "      <td>-2.850452e-01</td>\n",
              "      <td>0.277885</td>\n",
              "      <td>-0.548489</td>\n",
              "      <td>-0.926409</td>\n",
              "      <td>-6.890228e-01</td>\n",
              "      <td>-4.613141e-01</td>\n",
              "      <td>-0.625537</td>\n",
              "      <td>-0.585838</td>\n",
              "      <td>-7.097515e-01</td>\n",
              "      <td>-0.891636</td>\n",
              "      <td>-1.524249e+00</td>\n",
              "      <td>-0.901172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_068_S_4859_bl_0</th>\n",
              "      <td>-0.721446</td>\n",
              "      <td>0.969132</td>\n",
              "      <td>-1.003464</td>\n",
              "      <td>-1.566753</td>\n",
              "      <td>-1.333146</td>\n",
              "      <td>-1.154884</td>\n",
              "      <td>-0.981239</td>\n",
              "      <td>0.016152</td>\n",
              "      <td>0.054104</td>\n",
              "      <td>-0.068596</td>\n",
              "      <td>0.130535</td>\n",
              "      <td>0.317595</td>\n",
              "      <td>0.139523</td>\n",
              "      <td>1.282840</td>\n",
              "      <td>0.998632</td>\n",
              "      <td>-0.499798</td>\n",
              "      <td>-0.942494</td>\n",
              "      <td>-0.707248</td>\n",
              "      <td>0.544950</td>\n",
              "      <td>0.269282</td>\n",
              "      <td>0.396732</td>\n",
              "      <td>-1.085951</td>\n",
              "      <td>-0.391107</td>\n",
              "      <td>-0.168379</td>\n",
              "      <td>-0.350802</td>\n",
              "      <td>0.201779</td>\n",
              "      <td>-0.084476</td>\n",
              "      <td>0.015409</td>\n",
              "      <td>0.347762</td>\n",
              "      <td>-0.177245</td>\n",
              "      <td>0.643651</td>\n",
              "      <td>0.317883</td>\n",
              "      <td>-0.570530</td>\n",
              "      <td>0.564968</td>\n",
              "      <td>0.495168</td>\n",
              "      <td>1.384488</td>\n",
              "      <td>-0.910762</td>\n",
              "      <td>-0.315239</td>\n",
              "      <td>-0.261624</td>\n",
              "      <td>-0.904765</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>7.722062e-01</td>\n",
              "      <td>0.561204</td>\n",
              "      <td>-0.430410</td>\n",
              "      <td>-0.705520</td>\n",
              "      <td>1.523412</td>\n",
              "      <td>-1.163021</td>\n",
              "      <td>-1.187733</td>\n",
              "      <td>-0.987593</td>\n",
              "      <td>-0.820054</td>\n",
              "      <td>-0.654166</td>\n",
              "      <td>-0.680954</td>\n",
              "      <td>-0.973399</td>\n",
              "      <td>-1.177733</td>\n",
              "      <td>-1.376789</td>\n",
              "      <td>-1.172258</td>\n",
              "      <td>-0.705520</td>\n",
              "      <td>1.523412</td>\n",
              "      <td>-1.163021</td>\n",
              "      <td>-1.187733</td>\n",
              "      <td>-0.987593</td>\n",
              "      <td>-0.820054</td>\n",
              "      <td>-0.654166</td>\n",
              "      <td>-0.680954</td>\n",
              "      <td>-0.973399</td>\n",
              "      <td>-1.177733</td>\n",
              "      <td>-1.376789</td>\n",
              "      <td>-1.172258</td>\n",
              "      <td>-4.820212e-02</td>\n",
              "      <td>1.020540</td>\n",
              "      <td>-0.315334</td>\n",
              "      <td>-0.056443</td>\n",
              "      <td>2.091517e-01</td>\n",
              "      <td>3.679540e-01</td>\n",
              "      <td>0.041160</td>\n",
              "      <td>-0.143128</td>\n",
              "      <td>2.018396e-01</td>\n",
              "      <td>-0.110809</td>\n",
              "      <td>-5.495174e-01</td>\n",
              "      <td>-0.175953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0_014_S_4577_bl_0</th>\n",
              "      <td>-0.818894</td>\n",
              "      <td>0.273066</td>\n",
              "      <td>-0.305148</td>\n",
              "      <td>-1.345541</td>\n",
              "      <td>-0.997132</td>\n",
              "      <td>-0.992494</td>\n",
              "      <td>-0.859142</td>\n",
              "      <td>-0.086819</td>\n",
              "      <td>-0.339727</td>\n",
              "      <td>-0.040461</td>\n",
              "      <td>-0.387104</td>\n",
              "      <td>-0.176845</td>\n",
              "      <td>-0.417554</td>\n",
              "      <td>0.516218</td>\n",
              "      <td>0.848233</td>\n",
              "      <td>-0.899843</td>\n",
              "      <td>-0.587409</td>\n",
              "      <td>-1.157821</td>\n",
              "      <td>0.447955</td>\n",
              "      <td>0.230946</td>\n",
              "      <td>1.200228</td>\n",
              "      <td>-1.084410</td>\n",
              "      <td>-0.629236</td>\n",
              "      <td>0.411978</td>\n",
              "      <td>0.325904</td>\n",
              "      <td>0.148354</td>\n",
              "      <td>0.504006</td>\n",
              "      <td>0.191490</td>\n",
              "      <td>0.380666</td>\n",
              "      <td>-0.144670</td>\n",
              "      <td>0.587432</td>\n",
              "      <td>-0.194931</td>\n",
              "      <td>-0.862502</td>\n",
              "      <td>0.322115</td>\n",
              "      <td>0.350552</td>\n",
              "      <td>1.362691</td>\n",
              "      <td>-1.138557</td>\n",
              "      <td>-0.542710</td>\n",
              "      <td>-0.769328</td>\n",
              "      <td>-0.864697</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.253095</td>\n",
              "      <td>4.274007e-01</td>\n",
              "      <td>-0.142625</td>\n",
              "      <td>-0.414234</td>\n",
              "      <td>-0.804021</td>\n",
              "      <td>0.584499</td>\n",
              "      <td>-0.530647</td>\n",
              "      <td>-1.305337</td>\n",
              "      <td>-1.131941</td>\n",
              "      <td>-0.867685</td>\n",
              "      <td>-0.513624</td>\n",
              "      <td>-0.849045</td>\n",
              "      <td>-1.175492</td>\n",
              "      <td>-1.302781</td>\n",
              "      <td>-0.816064</td>\n",
              "      <td>-1.301305</td>\n",
              "      <td>-0.804021</td>\n",
              "      <td>0.584499</td>\n",
              "      <td>-0.530647</td>\n",
              "      <td>-1.305337</td>\n",
              "      <td>-1.131941</td>\n",
              "      <td>-0.867685</td>\n",
              "      <td>-0.513624</td>\n",
              "      <td>-0.849045</td>\n",
              "      <td>-1.175492</td>\n",
              "      <td>-1.302781</td>\n",
              "      <td>-0.816064</td>\n",
              "      <td>-1.301305</td>\n",
              "      <td>-1.280293e-01</td>\n",
              "      <td>0.768279</td>\n",
              "      <td>0.298065</td>\n",
              "      <td>0.277205</td>\n",
              "      <td>-3.600648e-02</td>\n",
              "      <td>1.459646e-02</td>\n",
              "      <td>-0.242724</td>\n",
              "      <td>-0.180020</td>\n",
              "      <td>-3.907054e-01</td>\n",
              "      <td>0.292521</td>\n",
              "      <td>5.284201e-01</td>\n",
              "      <td>-0.051641</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>598 rows × 261 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  hf_energy  ... vf_Frangi_inner_std_SR(1.0, 10.0)_SS2.0\n",
              "ID                           ...                                        \n",
              "0_021_S_0642_bl_0 -0.160235  ...                                0.718112\n",
              "0_135_S_4566_bl_0 -0.151509  ...                               -1.061634\n",
              "0_051_S_1123_bl_0 -0.529985  ...                                0.000000\n",
              "0_073_S_0311_bl_0 -0.577616  ...                               -0.525310\n",
              "0_037_S_4308_bl_0 -0.299787  ...                               -0.512715\n",
              "...                     ...  ...                                     ...\n",
              "0_033_S_5235_bl_0 -0.009206  ...                                0.096168\n",
              "0_052_S_1251_bl_0  2.997111  ...                               -0.684797\n",
              "0_082_S_4208_bl_0  0.209651  ...                               -0.901172\n",
              "0_068_S_4859_bl_0 -0.721446  ...                               -0.175953\n",
              "0_014_S_4577_bl_0 -0.818894  ...                               -0.051641\n",
              "\n",
              "[598 rows x 261 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDUa0jAipO-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code for feature selection\n",
        "'''\n",
        "First determine the best amount of features\n",
        "'''\n",
        "\n",
        "# Create the RFE object and compute a cross-validated score.\n",
        "svc = svm.SVC(kernel=\"linear\", random_state= None)\n",
        "X= X_train_scaled\n",
        "y= y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HijFXdeGgjTd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "3275f451-39ad-4983-d545-842b01570f72"
      },
      "source": [
        "\n",
        "\n",
        "# classifications\n",
        "rfecv = feature_selection.RFECV(\n",
        "    estimator=svc, step=1, \n",
        "    cv=model_selection.StratifiedKFold(n_splits=5),\n",
        "    scoring='roc_auc')\n",
        "rfecv.fit(X, y)\n",
        "feature_selection.RFECV()\n",
        "# Plot number of features VS. cross-validation scores\n",
        "plt.figure()\n",
        "plt.xlabel(\"Number of features selected\")\n",
        "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
        "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
        "plt.show()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-710fb2c7816c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     scoring='roc_auc')\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mrfecv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRFECV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Plot number of features VS. cross-validation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    531\u001b[0m         scores = parallel(\n\u001b[1;32m    532\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    531\u001b[0m         scores = parallel(\n\u001b[1;32m    532\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrfe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_rfe_single_fit\u001b[0;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     return rfe._fit(\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         _score(estimator, X_test[:, features], y_test, scorer)).scores_\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_rfe.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, step_score)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting estimator with %d features.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# Get coefs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G99ms2zwhnpX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "outputId": "cd293248-f8fb-476c-bd0a-2d9e3ff60b4f"
      },
      "source": [
        "# Code for feature selection\n",
        "'''\n",
        "With the amount of features desired known, find the best performing features\n",
        "'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_classif\n",
        " \n",
        "#apply SelectKBest class to extract top ... best features\n",
        "bestfeatures = SelectKBest(score_func= f_classif, k=32) # k= number of desired features\n",
        "fit = bestfeatures.fit(X,y)\n",
        "dfscores = pd.DataFrame(fit.scores_)\n",
        "dfcolumns = pd.DataFrame(X.columns)\n",
        "#concat two dataframes for better visualization \n",
        "featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
        "featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
        "print(featureScores.nlargest(32,'Score'))  #print ... best features \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            Specs       Score\n",
            "1                                   (hf_entropy,)  540.044084\n",
            "2                                  (hf_kurtosis,)  404.552131\n",
            "11                                      (hf_std,)  364.426795\n",
            "8                            (hf_quartile_range,)  353.618383\n",
            "218                          (tf_LBP_std_R3_P12,)  349.047098\n",
            "99   (tf_GLRLM_GrayLevelNonUniformityNormalized,)  282.934329\n",
            "100                 (tf_GLRLM_GrayLevelVariance,)  281.213410\n",
            "111                  (tf_GLRLM_ShortRunEmphasis,)  280.321212\n",
            "222                        (tf_NGTDM_Complexity,)  279.807015\n",
            "113      (tf_GLRLM_ShortRunLowGrayLevelEmphasis,)  257.356041\n",
            "73              (phasef_phasesym_entropy_WL3_N5,)  238.773800\n",
            "134                      (tf_Gabor_0.05A0.0skew,)  226.395155\n",
            "6                                       (hf_min,)  207.733146\n",
            "150                     (tf_Gabor_0.05A2.36mean,)  188.879981\n",
            "9                                     (hf_range,)  187.038136\n",
            "79             (phasef_phasesym_skewness_WL3_N5,)  185.437295\n",
            "102                   (tf_GLRLM_LongRunEmphasis,)  183.988293\n",
            "223                          (tf_NGTDM_Contrast,)  183.260703\n",
            "109                     (tf_GLRLM_RunPercentage,)  183.125864\n",
            "76                 (phasef_phasesym_mean_WL3_N5,)  177.247287\n",
            "139                      (tf_Gabor_0.05A0.79min,)  175.850030\n",
            "162                      (tf_Gabor_0.2A0.79mean,)  168.068926\n",
            "203                     (tf_LBP_kurtosis_R3_P12,)  167.987448\n",
            "137                      (tf_Gabor_0.05A0.79max,)  162.962924\n",
            "80                  (phasef_phasesym_std_WL3_N5,)  161.997513\n",
            "75                  (phasef_phasesym_max_WL3_N5,)  161.792751\n",
            "78                (phasef_phasesym_range_WL3_N5,)  161.792751\n",
            "155                        (tf_Gabor_0.2A0.0max,)  161.561995\n",
            "220                          (tf_NGTDM_Busyness,)  153.628289\n",
            "159                        (tf_Gabor_0.2A0.0std,)  151.270528\n",
            "154                       (tf_Gabor_0.2A0.0kurt,)  146.074870\n",
            "107            (tf_GLRLM_RunLengthNonUniformity,)  145.674808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:114: UserWarning: Features [208 211 212 213] are constant.\n",
            "  UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_selection/_univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n",
            "  f = msb / msw\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1mm5LEp7m4Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "eaa28d85-93fd-4aaf-89a9-8f1a6262ae55"
      },
      "source": [
        "# Code for PCA\n",
        "from sklearn import decomposition\n",
        "pca = decomposition.PCA(n_components=260)\n",
        "X= X_train_scaled\n",
        "test= X_test_scaled\n",
        "pca.fit(X)\n",
        "X_train_pca = pca.transform(X)\n",
        "y= y_train\n",
        "# X_pca= pd.DataFrame(data= X_train_pca, index=[X.index], columns=[X.columns])\n",
        "X_test_pca = pca.transform(test)\n",
        "print(X_train_pca)\n",
        "#X_train_scaled = pd.DataFrame(data=X_train_scaled, index=[X_train.index], columns=[X_train.columns])\n",
        "seaborn.scatterplot(x=X_train_pca[:, 0], y=X_train_pca[:, 1], hue=y)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-56ba5eb5f932>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecomposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_train_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX_test_scaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train_scaled' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3cuKF5t6Gdt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance');"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}